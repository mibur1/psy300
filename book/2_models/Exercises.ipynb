{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5930baa2",
   "metadata": {},
   "source": [
    "# <i class=\"fa-solid fa-dumbbell\"></i> Exercises\n",
    "\n",
    "Please fill the missing code pieces as indicated by the `...`. The imports are always provided at the top of the code chunks. This should give you a hint for which functions/classes to use. Have a look at the online documentation if you are unsure how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0081c1",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# TODO: Get data\n",
    "\n",
    "# TODO: Extract features and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d510db",
   "metadata": {},
   "source": [
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51272717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Fit and evaluare regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3648e8d",
   "metadata": {},
   "source": [
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Split data into training and test sets\n",
    "\n",
    "# TODO: Perform forward sequential feature selection\n",
    "sfs_forward = ...\n",
    "\n",
    "# Summary of results\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c2d4d",
   "metadata": {},
   "source": [
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the selected features as a list\n",
    "\n",
    "# TODO: Train and evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29057",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "\n",
    "# TODO: Drop highly correlated features and rows with missing data\n",
    "...\n",
    "\n",
    "# TODO: Get the target (y) and features (X), then split into training and test set\n",
    "...\n",
    "\n",
    "# TODO: Scale predictors to mean=0 and std=1\n",
    "...\n",
    "\n",
    "# TODO: Implement Lasso \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cafe4",
   "metadata": {},
   "source": [
    "## Exercise 3: GAMs (1)\n",
    "\n",
    "Objective: Understand how the number of basis functions (df) and the polynomial degree (degree) affect the flexibility of a spline and the resulting fit in a Generalized Additive Model.\n",
    "\n",
    "1. Use the diabetes dataset and focus on the relationship between `bmi` and `target`.\n",
    "2. We want to test different combinations of parameters. For the dfs, please use 4, 6, 12. For the degree, please use 2 and 3 (quadratic and cubic).\n",
    "3. Fit the GAMs for each parameter combination. The resulting models will be plotted automatically for visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "\n",
    "# TODO: 1. Get bmi as x and the target as y\n",
    "data = load_diabetes(as_frame=True)\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "# TODO: 2. Define possible parameters\n",
    "...\n",
    "\n",
    "# TODO: 3. Plot partial effect for each combination of df and degree\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a379c7",
   "metadata": {},
   "source": [
    "## Exercise 4: GAMs (2)\n",
    "\n",
    "We now use the [wage](https://islp.readthedocs.io/en/latest/datasets/Wage.html) dataset, which contains income information for a group of workers, along with demographic and employment-related features such as age, education, marital status, and job class.\n",
    "\n",
    "1) Explore the dataset\n",
    "    - Which variables are numeric?\n",
    "    - Which ones are categorical?\n",
    "\n",
    "2) Fit a GAM predicting `wage` from `age`, `year`, `education`, `jobclass`, and `maritl`\n",
    "\n",
    "Note: For categorical features we use a one-hot encoding with `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec422ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ISLP import load_data\n",
    "from statsmodels.gam.api import GLMGam, BSplines\n",
    "\n",
    "# Load data\n",
    "Wage = load_data('Wage')\n",
    "\n",
    "# TODO: Get continuous features\n",
    "smoooth_features = ...\n",
    "X_spline = Wage[smoooth_features]\n",
    "\n",
    "# TODO: Get categorical features — one-hot encode\n",
    "categoricals = ...\n",
    "X_cat = pd.get_dummies(Wage[categoricals], drop_first=True)\n",
    "\n",
    "# TODO: Define target variable\n",
    "y = ...\n",
    "\n",
    "# TODO: Create BSpline basis\n",
    "...\n",
    "\n",
    "# TODO: Fit GAM and print summary\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72059029",
   "metadata": {},
   "source": [
    "## Exercise 5: KNN\n",
    "\n",
    "You will implement a K-Nearest Neighbours (KNN) classifier to predict whether a patient is likely to have malignant or benign breast cancer based on several features. The data is already loaded for you, but please have a look a the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) to quickly refresh you memory about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af29c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# TODO: Create a combined DataFrame for easier inspection and manipulation\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f987610",
   "metadata": {},
   "source": [
    "Please implement the following:\n",
    "\n",
    "1. Subset the dataframe to use `mean area`, `mean radius`, and `mean smoothness` as features (X), and `target` as the target (y)\n",
    "2. Scale the predictors to mean 0 and variance 1\n",
    "3. Split the data into a training and a testing set (70/30)\n",
    "4. Train a kNN classifier with $k=5$\n",
    "5. Evaluate the performance for the testing set. Please use the `accuracy_score()` as well as the `classification_report()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. Select features and target\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# TODO: 2. Scale the features\n",
    "scaler = ...\n",
    "X_scaled = ...\n",
    "\n",
    "# TODO: 3. Split the data into training and testing sets\n",
    "...\n",
    "#  TODO: 4. Perform KNN classification\n",
    "...\n",
    "\n",
    "# TODO: 5. Get predictions\n",
    "...\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(...))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae50387",
   "metadata": {},
   "source": [
    "The classification model from the previous step has two main limitations:\n",
    "\n",
    "1. It is trained and evaluated on a single data split\n",
    "2. It uses a single $k$ even though we do not know if it is optimal\n",
    "\n",
    "Please do the following:\n",
    "\n",
    "1. Implement 5-fold cross validation\n",
    "2. Train models for $k$ ranging from 1 to 200 and plot the mean accuracy over all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "k_values = range(1, 200)\n",
    "mean_accuracies = []\n",
    "\n",
    "# TODO: 5-fold cross-validation for different k values\n",
    "...\n",
    "\n",
    "# TODO: Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(...)\n",
    "ax.set(xlabel=..., ylabel=..., title=...);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc67b0",
   "metadata": {},
   "source": [
    "Discuss the results. How is the performance in general? Which $k$ would you chose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4de454",
   "metadata": {},
   "source": [
    "## Exercise 6: LDA, QDA & Naïve Bayes\n",
    "\n",
    "Once again, we will use the Iris dataset for classificationa analysis. Your task is to compare the performance of LDA, QDA, and Gaussian Naïve Bayes!\n",
    "\n",
    "1. Load the `iris` dataset from `sklearn.datasets`. We will use only the first two features (sepal length and width)\n",
    "2. `TODO:` Split the data into training and test sets ([use stratification!](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))\n",
    "3. `TODO:` Fit LDA, QDA, and Naïve Bayes classifiers to the training data and orint the classification report for all models on the test data\n",
    "4. Plot the decision boundaries for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f54c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# 1. Load data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# 2. Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 3. TODO: Fit a LDA model and print the classification report\n",
    "lda = ...\n",
    "\n",
    "print(classification_report(y_test, lda.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47376e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# 3. TODO: Fit a QDA model and print the classification report\n",
    "qda = ...\n",
    "\n",
    "print(classification_report(y_test, qda.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 3. TODO: Fit a Gaussian Naive Bayes model and print the classification report\n",
    "gnb = ...\n",
    "\n",
    "print(classification_report(y_test, gnb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc3321",
   "metadata": {},
   "source": [
    "Once you have trained all three models, you can simply run the following code to plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot the decision boundaries for all 3 classifiers\n",
    "\n",
    "# Plotting function\n",
    "def plot_decision_boundary(model, X, y, title, ax):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.2)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=30)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "\n",
    "# Create plots for all 3 classifiers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "plot_decision_boundary(lda, X_train, y_train, \"LDA Decision Boundary\", axes[0])\n",
    "plot_decision_boundary(qda, X_train, y_train, \"QDA Decision Boundary\", axes[1])\n",
    "plot_decision_boundary(gnb, X_train, y_train, \"Naïve Bayes Decision Boundary\", axes[2])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83300c",
   "metadata": {},
   "source": [
    "## Exercise 7: Trees\n",
    "\n",
    "1. Inspect the data\n",
    "    - How many features are there and what are they?\n",
    "    - What is the target?\n",
    "\n",
    "2. Split the data into a train and test set, and make sure the classes are equally distributed (`stratify=y`)\n",
    "\n",
    "3. Fit the DecisionTreeClassifier(max_depth=3) and report train vs. test accuracy.\n",
    "\n",
    "4. Tree inspection (discuss in group)\n",
    "    - After fitting the model, the tree will be plotted automatically\n",
    "    - What is the very first split (feature name and threshold)?\n",
    "    - Which leaf nodes are pure, and which have mixed classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409445b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Load and inspect data\n",
    "diab = fetch_openml(\"diabetes\", version=1, as_frame=True)\n",
    "X = diab.data\n",
    "y = diab.target\n",
    "\n",
    "print(...)\n",
    "\n",
    "# 2) Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# 3) Fit tree\n",
    "clf = DecisionTreeClassifier(...)\n",
    "clf.fit(...)\n",
    "\n",
    "print(\"\\nTrain accuracy:\", accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test,  clf.predict(X_test)))\n",
    "\n",
    "# 5) Plot tree\n",
    "plt.figure(figsize=(14,7))\n",
    "plot_tree(clf, feature_names=X.columns, class_names=[\"neg\",\"pos\"], filled=True, rounded=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d61fb",
   "metadata": {},
   "source": [
    "Let's see if we can improve the classification performance with a random forest classifier and hyperparameter tuning!\n",
    "\n",
    "1. Set up the clasifier + a parameter grid for grid search with 5-fold CV\n",
    "    - n_estimators: 50, 100, 200\n",
    "    - max_depth: None, 10, 20\n",
    "    - min_samples_split: 2, 5, 10\n",
    "    - max_features: \"sqrt\", \"log2\", 0.5\n",
    "\n",
    "2. Fit the model with the grid search\n",
    "\n",
    "3. Print the best hyperparameters\n",
    "\n",
    "4. Evaluate the best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c895097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1) Set up Random Forest + parameter grid\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators':      ...,\n",
    "    'max_depth':         ...,\n",
    "    'min_samples_split': ...,\n",
    "    'max_features':      ...\n",
    "}\n",
    "\n",
    "# 2) Fit on training data\n",
    "grid = GridSearchCV(...)\n",
    "grid.fit(...)\n",
    "\n",
    "# 3) Print best hyperparameters\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(f\"CV accuracy: {grid.best_score_:.3f}\")\n",
    "\n",
    "# 4) Evaluate on the held‐out test set\n",
    "best_rf = grid.best_estimator_\n",
    "y_pred = ...\n",
    "\n",
    "print(f\"\\nTest accuracy: {accuracy_score(y_test, y_pred):.3f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['neg','pos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f256bd",
   "metadata": {},
   "source": [
    "## Exercise 8: SVC\n",
    "\n",
    "For the SVC exercise we will use the `fmri` dataset from `seaborn`, which contains measurements of brain activity (`signal`) in two brain regions (`frontal` and `parietal`) under two event types (`stim` vs. `cue`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e36e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset(\"fmri\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cddbd4",
   "metadata": {},
   "source": [
    "We will try to answer a very simple research question:\n",
    "\n",
    "> Can we distinguish between `cue` and `stim` events based on the fMRI signal in the `parietal` and `frontal` brain regions?\n",
    "\n",
    "To do this, we need to turn the long‐format data into a classic “feature matrix” (one row = one sample, two columns = our two brain‐region signals) plus a corresponding label vector (cue/stim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df.pivot_table(\n",
    "    index=[\"subject\",\"timepoint\",\"event\"],\n",
    "    columns=\"region\",\n",
    "    values=\"signal\"\n",
    ").reset_index()\n",
    "df_wide.columns.name = None\n",
    "\n",
    "X = df_wide[[\"frontal\",\"parietal\"]] \n",
    "y = df_wide[\"event\"].map({\"cue\":0,\"stim\":1})\n",
    "\n",
    "print(\"\\nFeatures:\")\n",
    "print(X)\n",
    "print(\"\\nTarget:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505936fb",
   "metadata": {},
   "source": [
    "With the features and target in the correct form, please perform the following tasks:\n",
    "\n",
    "1. Split the data into a train and test set\n",
    "2. Scale the predictors to mean 0 and std 1\n",
    "3. Fit a linear as well as a rbf SVC and **discuss the classification reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81285f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. TODO: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# 2. TODO: Scale the features after splitting (important to avoid data leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(...)\n",
    "X_test_sc  = scaler.transform(...)\n",
    "\n",
    "# 3. TODO: Fit the SVC models and compare the classification reports\n",
    "clf_lin = SVC(...)\n",
    "clf_lin.fit(...)\n",
    "y_pred_lin = clf_lin.predict(...)\n",
    "print(\"Linear SVC\\n\", classification_report(...))\n",
    "\n",
    "clf_rbf = SVC(...)\n",
    "clf_rbf.fit(...)\n",
    "y_pred_rbf = clf_rbf.predict(...)\n",
    "print(\"RBF SVC\\n\", classification_report(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860bd31",
   "metadata": {},
   "source": [
    "After fitting both models, you can run the code chunk below to plot the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad503497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None):\n",
    "    \"\"\"Plot the decision boundary for a trained 2D SVC model.\"\"\"\n",
    "    # Set up grid\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, 100), np.linspace(*ylim, 100))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    decision_values = model.decision_function(grid).reshape(xx.shape)\n",
    "    ax.contour(xx, yy, decision_values, levels=[0], linestyles=['-'], colors='k')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', linestyle='None', markersize=8, label='Cue', markerfacecolor=\"#0173B2\", markeredgecolor='None'),\n",
    "    Line2D([0], [0], marker='o', linestyle='None', markersize=8, label='Stim', markerfacecolor=\"#DE8F05\", markeredgecolor='None'),\n",
    "    Line2D([0], [0], color='k', linestyle='-', label='Decision boundary')]\n",
    "\n",
    "# Linear SVC\n",
    "sns.scatterplot(x = X_train_sc[:, 0], y = X_train_sc[:, 1], hue = y_train.map({0:\"cue\",1:\"stim\"}), palette = [\"#0173B2\", \"#DE8F05\"], s = 60, ax = ax[0], legend=None)\n",
    "ax[0].set(xlabel = \"Frontal signal (scaled)\", ylabel = \"Parietal signal (scaled)\", title  = \"Linear SVC Decision Boundary\")\n",
    "plot_svc_decision_function(clf_lin, ax=ax[0])\n",
    "ax[0].legend(handles=legend_elements, loc=\"upper left\", handlelength=1)\n",
    "\n",
    "# RBF SVC\n",
    "sns.scatterplot(x = X_train_sc[:, 0], y = X_train_sc[:, 1], hue = y_train.map({0:\"cue\",1:\"stim\"}), palette = [\"#0173B2\", \"#DE8F05\"], s = 60, ax = ax[1], legend=None)\n",
    "ax[1].set(xlabel = \"Frontal signal (scaled)\", ylabel = \"Parietal signal (scaled)\", title  = \"RBF SVC Decision Boundary\")\n",
    "plot_svc_decision_function(clf_rbf, ax=ax[1])\n",
    "ax[1].legend(handles=legend_elements, loc=\"upper left\", handlelength=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82ee91",
   "metadata": {},
   "source": [
    "Training a SVC on more complex datasets usually requires a parameter search to find the optimal hyperparameters. Please implement a grid search with the following options:\n",
    "\n",
    "- Kernel:   rbf\n",
    "- C:        np.logspace(-2,2,5)\n",
    "- gamma:    np.logspace(-3,1,5)\n",
    "- cv:       5-fold\n",
    "- scoring:  accuracy\n",
    "\n",
    "Print the optimal parameters and the corresponding accuracies for taining and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    ...\n",
    "}\n",
    "grid = GridSearchCV(...)\n",
    "grid.fit(...)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"CV accuracy:\", grid.best_score_)\n",
    "print(\"Test accuracy:\", grid.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eabdb",
   "metadata": {},
   "source": [
    "## Exercise 9: Neural Networks\n",
    "\n",
    "In this exercise, you will use `tensorflow` to create a single layer neural network to classify handwritten numbers from 0 to 9 from the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "*Hint: Tensorflow is one of the most widely used machine learning learning libraries. It was initially developed by Google, but is open source and available for everyone. Tensorflow requires Python <=3.12. If you have an environment with Python 3.13, you either need to create a new one or simply use Google Colab for this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data and plot examples\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "fig, ax = plt.subplots(1,5)\n",
    "for i in range(5):\n",
    "    ax[i].imshow(x_train[i], cmap='gray')\n",
    "    ax[i].set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60f446",
   "metadata": {},
   "source": [
    "We can then create the network with the following characteristics:\n",
    "\n",
    "- **Input**: A flattened version of the MNIST image (a vector of size 784)\n",
    "- **Architecture**: A single dense (fully connected) layer with 10 neurons (one for each class)\n",
    "- **Activation function**: `softmax`(outputs probabilities summing to 1)\n",
    "- **Output**: A probability distribution over digits 0–9; the highest is chosen\n",
    "- **Learning rule**: `categorical_crossentropy` loss and stochastic gradient descent (`SGD`) optimiser\n",
    "- **Evaluation metric**: `accuracy`, measuring the percentage of correctly classified images\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Explore the code and try to understand what it does (change things and see how they affect the result!)\n",
    "\n",
    "2. Improve the model to achieve a better predicion accuracy (>97%). Potential change you can make:\n",
    "    - Change the number of epochs or batch size (the number of training examples processed at once before the model weights are updated)\n",
    "    - Change the learning rate or optimiser (use e.g. Adam, which uses an adaptive learning rate and is faster)\n",
    "    - Change the model structure by e.g. adding a hidden layer with e.g. 64 or 128 neurons and a ReLu activation function\n",
    "\n",
    "3. Compare your model with other students. Who managed to get the highest testing accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2da2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# 1) Load and preprocess data (flatten & scale)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "x_test  = x_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "\n",
    "# 2) Create the model: One dense (fully connected) with a softmax activation function\n",
    "model = Sequential([\n",
    "    Input(shape=(784,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 3) Compile & train the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(learning_rate=0.01), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=1)\n",
    "\n",
    "# 4) Evaluate the model\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb2110",
   "metadata": {},
   "source": [
    "## Exercise 10: Principal Component Analysis\n",
    "\n",
    " For today’s practical session, we will work with the **Diabetes** dataset built into `scikit-learn`. This dataset contains medical information from 442 diabetes patients:\n",
    "\n",
    "* **Features (X):** 10 baseline variables (age, sex, BMI, average blood pressure, and six blood serum measures).\n",
    "* **Target (y):** a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "You can read more here: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load\\_diabetes.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Inspect & clean (already implemented)**\n",
    "\n",
    "   * Display summary statistics (`df.describe()`) for all 10 features.\n",
    "   * Check for missing values. (Hint: this dataset has none, but verify.)\n",
    "\n",
    "2. **Standardize**\n",
    "\n",
    "   * Use `StandardScaler()` to transform each feature to mean 0, variance 1.\n",
    "\n",
    "3. **PCA & scree plot**\n",
    "\n",
    "   * Fit `PCA()` to the standardized feature matrix.\n",
    "   * Plot the **explained variance ratio** for each principal component (a scree plot).\n",
    "   * Decide how many components to retain (e.g.\\ cumulative variance ≥ 80%).\n",
    "\n",
    "4. **Interpret loadings**\n",
    "\n",
    "   * Examine `pca.components_`.\n",
    "   * For the first two retained PCs, list the top 3 features by absolute loading.\n",
    "   * Infer what physiological patterns these components might represent.\n",
    "\n",
    "5. **Project the data for visualization**\n",
    "\n",
    "   * Compute the PCA projection: `X_pca = pca.transform(X_std)`.\n",
    "\n",
    "6. **Plot the results (already implemented)**\n",
    "   * Create a 2D scatter of PC1 vs. PC2, coloring points by whether the target is **above** or **below** the median progression value.\n",
    "   * Do patients with more rapid progression cluster differently?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beacf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the data as a DataFrame\n",
    "diabetes = load_diabetes(as_frame=True)\n",
    "df = diabetes.frame\n",
    "df.rename(columns={'target': 'Disease progression'}, inplace=True)\n",
    "\n",
    "X = df.drop(columns='Disease progression')\n",
    "y = df['Disease progression']\n",
    "\n",
    "# 1. Inspect the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 2. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = ...\n",
    "\n",
    "# 3. Perform the PCA\n",
    "pca = ...\n",
    "\n",
    "# 4. Get the explained variance ratio\n",
    "explained_variance = ...\n",
    "\n",
    "# 5. Project into PCA space\n",
    "X_pca = ...\n",
    "\n",
    "# 6. Plot the explained variance and 2D PCA projection\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(np.arange(1, len(explained_variance)+1), explained_variance.cumsum(), marker='o')\n",
    "ax[0].set(xlabel='Number of Components', ylabel='Cumulative Explained Variance', title='Scree Plot')\n",
    "\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', alpha=0.6, ax=ax[1])\n",
    "ax[1].set(xlabel='Principal Component 1', ylabel='Principal Component 2');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfc-multiverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
