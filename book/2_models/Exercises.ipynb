{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5930baa2",
   "metadata": {},
   "source": [
    "# <i class=\"fa-solid fa-dumbbell\"></i> Exercises\n",
    "\n",
    "Please fill the missing code pieces as indicated by the `...`. The imports are always provided at the top of the code chunks. This should give you a hint for which functions/classes to use. Have a look at the online documentation if you are unsure how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0081c1",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Selection\n",
    "\n",
    "Today we are working with the `California Housing dataset`, which you are already familiar with, as we previously used it while exploring resampling method.\n",
    "This dataset is based on the 1990 U.S. Census and includes features describing California districts. \n",
    "\n",
    "1) Familiarize yourself with the data\n",
    "    - What kind of features are in the dataset? What is the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# TODO: Get data\n",
    "...\n",
    "\n",
    "# TODO: Extract features and target\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d510db",
   "metadata": {},
   "source": [
    "2) Baseline model \n",
    "    - Create a baseline linear regression model using **all** features and evaluate the model through 5-fold cross validation, using R² as the performance metric\n",
    "    - Print the individual and average R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51272717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Fit and evaluare regression model\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3648e8d",
   "metadata": {},
   "source": [
    "3) Apply a forward stepwise selection to find a simpler suitable model.\n",
    "    - Split the data into 80% training data and 20% testing data (print the shape to confirm it was sucessful)\n",
    "    - Perform a forward stepwise selection with a linear regression model, 5-fold CV, R² score, and `parsimonious` feature selection (refer to [documentation](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) for further information)\n",
    "    - Print the best CV R² as well as the chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Split data into training and test sets\n",
    "...\n",
    "\n",
    "# TODO: Perform forward sequential feature selection\n",
    "sfs_forward = ...\n",
    "\n",
    "# Summary of results\n",
    "print(f\">> Forward SFS:\")\n",
    "print(f\"   Best CV R²      : {sfs_forward.k_score_:.3f}\")\n",
    "print(f\"   Optimal # feats : {len(sfs_forward.k_feature_idx_)}\")\n",
    "print(f\"   Feature names   : {sfs_forward.k_feature_names_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c2d4d",
   "metadata": {},
   "source": [
    "4) Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the selected features as a list\n",
    "...\n",
    "\n",
    "# TODO: Train and evaluate the model\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29057",
   "metadata": {},
   "source": [
    "## Exercise 2: LASSO\n",
    "\n",
    "Please implement a Lasso regression model similar to the Ridge model in the [Regularization](2_Regularization) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data related processing\n",
    "hitters = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\").data\n",
    "hitters_subset = hitters[[\"Salary\", \"AtBat\", \"Runs\",\"RBI\", \"CHits\", \"CAtBat\", \"CRuns\", \"CWalks\", \"Assists\", \"Hits\", \"HmRun\", \"Years\", \"Errors\", \"Walks\"]].copy()\n",
    "\n",
    "# TODO: Drop highly correlated features and rows with missing data\n",
    "...\n",
    "\n",
    "# TODO: Get the target (y) and features (X), then split into training and test set\n",
    "...\n",
    "\n",
    "# TODO: Scale predictors to mean=0 and std=1\n",
    "...\n",
    "\n",
    "# TODO: Implement Lasso \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0a637",
   "metadata": {},
   "source": [
    "## Exercise 3: Principal Component Analysis\n",
    "\n",
    " For today’s practical session, we will work with the **Diabetes** dataset built into `scikit-learn`. This dataset contains medical information from 442 diabetes patients:\n",
    "\n",
    "* **Features (X):** 10 baseline variables (age, sex, BMI, average blood pressure, and six blood serum measures).\n",
    "* **Target (y):** a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "You can read more here: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load\\_diabetes.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Inspect & clean (already implemented)**\n",
    "\n",
    "   * Display summary statistics (`df.describe()`) for all 10 features.\n",
    "   * Check for missing values. (Hint: this dataset has none, but verify.)\n",
    "\n",
    "2. **Standardize**\n",
    "\n",
    "   * Use `StandardScaler()` to transform each feature to mean 0, variance 1.\n",
    "\n",
    "3. **PCA & scree plot**\n",
    "\n",
    "   * Fit `PCA()` to the standardized feature matrix.\n",
    "   * Plot the **explained variance ratio** for each principal component (a scree plot).\n",
    "   * Decide how many components to retain (e.g.\\ cumulative variance ≥ 80%).\n",
    "\n",
    "4. **Interpret loadings**\n",
    "\n",
    "   * Examine `pca.components_`.\n",
    "   * For the first two retained PCs, list the top 3 features by absolute loading.\n",
    "   * Infer what physiological patterns these components might represent.\n",
    "\n",
    "5. **Project the data for visualization**\n",
    "\n",
    "   * Compute the PCA projection: `X_pca = pca.transform(X_std)`.\n",
    "\n",
    "6. **Plot the results (already implemented)**\n",
    "   * Create a 2D scatter of PC1 vs. PC2, coloring points by whether the target is **above** or **below** the median progression value.\n",
    "   * Do patients with more rapid progression cluster differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the data as a DataFrame\n",
    "diabetes = load_diabetes(as_frame=True)\n",
    "df = diabetes.frame\n",
    "df.rename(columns={'target': 'Disease progression'}, inplace=True)\n",
    "\n",
    "X = df.drop(columns='Disease progression')\n",
    "y = df['Disease progression']\n",
    "\n",
    "# 1. Inspect the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 2. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = ...\n",
    "\n",
    "# 3. Perform the PCA\n",
    "pca = ...\n",
    "\n",
    "# 4. Get the explained variance ratio\n",
    "explained_variance = ...\n",
    "\n",
    "# 5. Project into PCA space\n",
    "X_pca = ...\n",
    "\n",
    "# 6. Plot the explained variance and 2D PCA projection\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(, marker='o')\n",
    "ax[0].set(xlabel='Number of Components', ylabel='Cumulative Explained Variance', title='Scree Plot')\n",
    "\n",
    "sns.scatterplot(..., hue=y, palette='viridis', alpha=0.6, ax=ax[1])\n",
    "ax[1].set(xlabel='Principal Component 1', ylabel='Principal Component 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389910b",
   "metadata": {},
   "source": [
    "## Exercise 3.2: PCR and PLS\n",
    "\n",
    "In this exercise, we will compare PCR and PLS on the classic Diabetes dataset from scikit-learn. This dataset contains 10 baseline variables (age, BMI, blood pressure, etc.) and a quantitative target: a measure of disease progression one year after baseline.\n",
    "\n",
    "We start by loading the data and extracting the features (`X`) as well as the target (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dac302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_diabetes(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52902388",
   "metadata": {},
   "source": [
    "How many predictors does the dataset have? Are any of them obviously correlated? Visualize them with a correlation matrix/heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# TODO: Plot the correlation matrix with sns.heatmap()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ec4fb",
   "metadata": {},
   "source": [
    "Please apply PCR with a range of components to see how the performance changes.\n",
    "\n",
    "1. Use 10-fold CV\n",
    "2. Try 1 to 10 components\n",
    "3. Create a model pipeline with `make_pipeline()`\n",
    "4. Evaluate the models with `cross_val_score()`\n",
    "5. Plot the $R^2$ over the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Set up variables\n",
    "cv = ...\n",
    "n_components = ...\n",
    "pcr_scores = []\n",
    "\n",
    "# TODO: Loop over number of components, create pipeline, evaluate with cross_val_score()\n",
    "for n in n_components:\n",
    "    model = ...\n",
    "    scores = ...\n",
    "    pcr_scores.append(scores.mean())\n",
    "\n",
    "# TODO: Plot results\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf5431",
   "metadata": {},
   "source": [
    "Now, do the exact same thing with `PLSRegression`. How does PLS compare to PCR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c17616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "pls_scores = []\n",
    "\n",
    "# TODO: Loop over number of components, create PLS model, evaluate with cross_val_score()\n",
    "for n in n_components:\n",
    "    pls = ...\n",
    "    scores = ...\n",
    "    pls_scores.append(scores.mean())\n",
    "\n",
    "# TODO: Plot PLS and PCR results together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb291d5",
   "metadata": {},
   "source": [
    "## Exercise 4: Logistic Regression\n",
    "\n",
    "For today's exercise we will use the **Breast Cancer Wisconsin (Diagnostic)**. It is a collection of data used for predicting whether a breast tumor is malignant (cancerous) or benign (non-cancerous), containing information derived from images of breast mass samples obtained through fine needle aspirates.\n",
    "\n",
    "The dataset consists of 569 samples with 30 features that measure various characteristics of cell nuclei, such as radius, texture, perimeter, and area. Each sample is labeled as either **malignant (1)** or **benign (0)**.\n",
    "\n",
    "1. Please [visit the documentation](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) and familiarize yourself with the dataset\n",
    "2. Take an initial look at the features (predictors) and targets (outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a24fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# Get data (as pandas dataframes) \n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# Convert y to a 1D array (this is the required input for the logistic regression model)\n",
    "y = ...\n",
    "\n",
    "# Print some general information and statistics about the dataset \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f68dd3",
   "metadata": {},
   "source": [
    "1. Split the data into training and test sets (stratify `y`)\n",
    "2. Create and fit a baseline model using only 2-3 interpretable predictors of your choice\n",
    "    - Print the model coefficients\n",
    "3. Evaluate on the test set:\n",
    "    - Accuracy\n",
    "    - Confusion matrix\n",
    "    - Classification report\n",
    "    - Compare test accuracy to train accuracy. Is there a big gap?\n",
    "\n",
    "*Hint: If you get a warning about convergence, try setting `max_iter=10000` in the logistic regression class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1. Split the data\n",
    "...\n",
    "\n",
    "# 2. Baseline model with three features\n",
    "...\n",
    "\n",
    "# 3. Evaluate the baseline model\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728b591",
   "metadata": {},
   "source": [
    "4. Use all predictors and build a pipeline with standardisation\n",
    "5. Evaluate on the test set\n",
    "6. Compare:\n",
    "    - Baseline model vs full model (test accuracy, precision, recall for malignant)\n",
    "    - Is the full model clearly better?\n",
    "    - Is there any sign of overfitting (train vs test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 4. Full model with all features\n",
    "...\n",
    "\n",
    "# 5. Evaluate the full model\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa95644",
   "metadata": {},
   "source": [
    "6. Create a custom plot which visualizes the confusion matrix It should contain:\n",
    "    - The four squares of the matrix (color coded)\n",
    "    - Labels of the actual values in the middle of each square\n",
    "    - Labels for all squares\n",
    "    - A colorbar\n",
    "    - A title\n",
    "\n",
    "7. Use scikit-learn to do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot a custom confusion matrix\n",
    "...\n",
    "\n",
    "# 6. Use scikit-learn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4de454",
   "metadata": {},
   "source": [
    "## Exercise 5: LDA, QDA & Naïve Bayes\n",
    "\n",
    "Once again, we will use the Iris dataset for classificationa analysis. Your task is to compare the performance of LDA, QDA, and Gaussian Naïve Bayes!\n",
    "\n",
    "1. Load the `iris` dataset from `sklearn.datasets`. We will use only the first two features (sepal length and width)\n",
    "2. `TODO:` Split the data into training and test sets ([use stratification!](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))\n",
    "3. `TODO:` Fit LDA, QDA, and Naïve Bayes classifiers to the training data and orint the classification report for all models on the test data\n",
    "4. Plot the decision boundaries for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f54c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# 1. Load data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# 2. Split into train/test\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 3. TODO: Fit a LDA model and print the classification report\n",
    "lda = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47376e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# 3. TODO: Fit a QDA model and print the classification report\n",
    "qda = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 3. TODO: Fit a Gaussian Naive Bayes model and print the classification report\n",
    "gnb = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc3321",
   "metadata": {},
   "source": [
    "Once you have trained all three models, you can plot their decision boundaries based on the training data. For this, you can use the provided helper function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot the decision boundaries for all 3 classifiers\n",
    "\n",
    "# Helper function to plot decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title, ax):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.2)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=30)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "\n",
    "# Create plots for all 3 classifiers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "...  # LDA\n",
    "...  # QDA\n",
    "...  # Naïve Bayes\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
